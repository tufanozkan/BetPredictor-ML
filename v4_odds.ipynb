{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"-VTlK120J3Uj","outputId":"98580524-6d64-40e9-e2ae-e27598c3798e"},"outputs":[],"source":["!pip install transformers datasets accelerate bitsandbytes peft torch pandas scikit-learn huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyNmjk6tKe37"},"outputs":[],"source":["#import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\nfrom huggingface_hub import login\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch\nfrom google.colab import files\nimport io"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oW5gG_lUKnnP"},"outputs":[],"source":["#connect to drive and read csv files\nfrom google.colab import drive\n\ndrive.mount('/content/drive')\n\nhistory_path = \"/content/drive/My Drive/history.csv\"\nfuture_path = \"/content/drive/My Drive/upcoming.csv\"\n\nhistory_df = pd.read_csv(history_path, encoding=\"utf-8\", on_bad_lines='warn')\nfuture_df = pd.read_csv(future_path, encoding=\"utf-8\", on_bad_lines='warn')\n\n#check a few lines\nprint(\"History Data:\")\nprint(history_df.head())\n\nprint(\"\\nFuture Matches Data:\")\nprint(future_df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jt_RtMLULc9m"},"outputs":[],"source":["#process historical match data\ndef process_history_data(row):\n    \"\"\"Process historical match data with known results\"\"\"\n    try:\n        if 'ft_score' in row:\n            scores = row['ft_score'].split('-')\n            home_score = int(scores[0].strip())\n            away_score = int(scores[1].strip())\n\n            #determine result: 1 for home win, 0 for draw, 2 for away win\n            if home_score > away_score:\n                result = 1  #home win\n            elif home_score == away_score:\n                result = 0  #draw\n            else:\n                result = 2  #away win\n        elif 'result' in row:\n            #if result is directly available\n            result = int(row['result'])\n        else:\n            #default to placeholder if no result info is available\n            result = -1\n            print(f\"Warning: No result information for match {row['home_name']} vs {row['away_name']}\")\n    except Exception as e:\n        print(f\"Error processing match {row['home_name']} vs {row['away_name']}: {e}\")\n        result = -1\n\n    input_text = (\n        f\"{row['home_name']} vs {row['away_name']} - \"\n        f\"(Odds: 1={row['o_pre_1']}, 0={row['o_pre_0']}, 2={row['o_pre_2']})\"\n    )\n    return {\"input\": input_text, \"label\": result}\n\n\nhistory_dataset = [process_history_data(row) for _, row in history_df.iterrows()]\nprint(f\"Processed {len(history_dataset)} historical matches\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqFNFjkNMTG2"},"outputs":[],"source":["#process future match data\ndef process_future_data(row):\n    \"\"\"Process future match data with unknown results\"\"\"\n    odds_dict = {}\n\n    #collect all available odds\n    for col in row.index:\n        if col.startswith('o_') and not pd.isna(row[col]):\n            odds_dict[col] = row[col]\n\n    #format the odds string\n    odds_parts = []\n\n    #add basic match odds\n    if all(k in odds_dict for k in ['o_ft_1', 'o_ft_0', 'o_ft_2']):\n        odds_parts.append(f\"1={odds_dict['o_ft_1']}, 0={odds_dict['o_ft_0']}, 2={odds_dict['o_ft_2']}\")\n\n    #add GG odds if available\n    if all(k in odds_dict for k in ['o_kg_v', 'o_kg_y']):\n        odds_parts.append(f\"GG Yes={odds_dict['o_kg_v']}, GG No={odds_dict['o_kg_y']}\")\n\n    #add double chance odds if available\n    if all(k in odds_dict for k in ['o_double_1_0', 'o_double_1_2', 'o_double_0_2']):\n        odds_parts.append(f\"DC 1-0={odds_dict['o_double_1_0']}, 1-2={odds_dict['o_double_1_2']}, 0-2={odds_dict['o_double_0_2']}\")\n\n    #add total goals odds if available\n    total_goals = [k for k in odds_dict if k.startswith('o_total_')]\n    if total_goals:\n        total_parts = []\n        for k in total_goals:\n            label = k.replace('o_total_', '')\n            total_parts.append(f\"{label}={odds_dict[k]}\")\n        odds_parts.append(\"Goals: \" + \", \".join(total_parts))\n\n    #combine all parts\n    input_text = f\"{row['home_name']} vs {row['away_name']} - (Odds: {'; '.join(odds_parts)})\"\n\n    return {\"input\": input_text, \"label\": -1}  #use -1 to indicate unknown result\n\n\nfuture_dataset = [process_future_data(row) for _, row in future_df.iterrows()]\nprint(f\"Processed {len(future_dataset)} future matches\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPIuULlOMXm0"},"outputs":[],"source":["#create datasets\n#split historical data for training and validation\ntrain_data, val_data = train_test_split(history_dataset, test_size=0.2, random_state=42)\n\ndataset = DatasetDict({\n    \"train\": Dataset.from_list(train_data),\n    \"validation\": Dataset.from_list(val_data),\n    \"future\": Dataset.from_list(future_dataset)  #keep future matches separate\n})\n\nprint(f\"Train set: {len(dataset['train'])} examples\")\nprint(f\"Validation set: {len(dataset['validation'])} examples\")\nprint(f\"Future set: {len(dataset['future'])} examples\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5hZIxvIMfIZ"},"outputs":[],"source":["#hugging Face login and model setup\n\nfrom huggingface_hub import login\n#sign in hugging face\nlogin(token=\"YOUR_TOKEN\")\n\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\n\nprint(f\"Using model: {model_name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZu3GWfjM3Rt"},"outputs":[],"source":["#setup model and tokenizer\ndef setup_model_and_tokenizer(model_name):\n    \"\"\"Setup the model with 4-bit quantization and LoRA\"\"\"\n    print(f\"Loading model: {model_name}...\")\n\n    #4-bit quantization configuration\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16  #changed to float16 for Colab compatibility\n    )\n\n    #load model with 4-bit quantization\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\"\n    )\n\n    #LoRA adapter configuration - adjust target_modules based on model architecture\n    #different models have different module names\n    if \"llama\" in model_name.lower():\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n    elif \"phi\" in model_name.lower():\n        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"fc1\", \"fc2\"]\n    else:\n        target_modules = [\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n\n    lora_config = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=8,  #LoRA rank\n        lora_alpha=32,  #LoRA alpha\n        lora_dropout=0.1,  #dropout rate\n        target_modules=target_modules\n    )\n\n    #add LoRA adapter\n    model = get_peft_model(model, lora_config)\n\n    #load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    print(\"Model and tokenizer loaded successfully!\")\n    return model, tokenizer\n\n#initialize model and tokenizer\nmodel, tokenizer = setup_model_and_tokenizer(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-izeQKP2NnzP"},"outputs":[],"source":["#preprocess data for the model\ndef preprocess_data(examples, tokenizer):\n    \"\"\"Process examples in batches with tokenizer\"\"\"\n    #format inputs with instruction prompt\n    formatted_inputs = [\n        f\"Analyze this football match and predict the best betting option (1 for home win, 0 for draw, 2 for away win): {example}\"\n        for example in examples[\"input\"]\n    ]\n\n    #tokenize inputs\n    model_inputs = tokenizer(\n        formatted_inputs,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=256,\n        return_tensors=None  #changed this from \"pt\" to None - critical fix\n    )\n\n    #handle labels\n    if \"label\" in examples:\n        labels = examples[\"label\"]\n        #ensure all labels are properly formatted for the loss function\n        labels = [int(label) if label != -1 else -100 for label in labels]\n        model_inputs[\"labels\"] = labels\n\n    return model_inputs\n\n#process datasets\nprint(\"Processing datasets...\")\nprocessed_dataset = {}\nfor split in [\"train\", \"validation\"]:\n    processed_dataset[split] = dataset[split].map(\n        lambda examples: preprocess_data(examples, tokenizer),\n        batched=True,\n        remove_columns=dataset[split].column_names\n    )\nprint(\"Datasets processed successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lkgPTWqNwSe"},"outputs":[],"source":["#training setup and execution\ndef train_model(processed_dataset, model, tokenizer):\n    \"\"\"Train the model on the dataset\"\"\"\n    #print some information about the processed datasets\n    print(f\"Training dataset size: {len(processed_dataset['train'])}\")\n    print(f\"Validation dataset size: {len(processed_dataset['validation'])}\")\n\n    #check if datasets have the right format\n    example_input = processed_dataset[\"train\"][0]\n    print(f\"Example training input keys: {example_input.keys()}\")\n\n    #training arguments - adjusted for Colab\n    training_args = TrainingArguments(\n        output_dir=\"./betting-model\",\n        evaluation_strategy=\"epoch\",\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,\n        per_device_eval_batch_size=1,\n        num_train_epochs=2,\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        push_to_hub=False,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        fp16=True,\n        optim=\"adamw_torch\",\n        #added parameters to help with debugging\n        remove_unused_columns=False,  #important fix to retain all columns\n        report_to=\"none\",  #disable wandb and other reporting\n        disable_tqdm=False  #show progress bars\n    )\n\n    #initialize trainer with a data collator\n    from transformers import DataCollatorForLanguageModeling\n\n    #use a proper data collator for language modeling\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False  # We're not doing masked language modeling\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=processed_dataset[\"train\"],\n        eval_dataset=processed_dataset[\"validation\"],\n        tokenizer=tokenizer,\n        data_collator=data_collator  #added data collator\n    )\n\n    #train model\n    print(\"Starting training...\")\n    trainer.train()\n\n    #save model\n    print(\"Saving model...\")\n    trainer.save_model(\"./betting-model-final\")\n    print(\"Model saved to ./betting-model-final\")\n\n    return trainer\n\n#train the model\ntrainer = train_model(processed_dataset, model, tokenizer)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}